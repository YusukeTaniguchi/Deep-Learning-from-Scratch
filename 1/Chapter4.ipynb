{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09750000000000003"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "def mean_squared_error(y, t):\n",
    "    return 0.5 * np.sum((y - t) ** 2)\n",
    "\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "mean_squared_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.510825457099338"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.302584092994546"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46763, 32263, 45632,  8080, 43996, 22268, 31450, 32747, 48676,\n",
       "       47046])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = t.reshape(1, y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7))/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def function_1(x):\n",
    "    return 0.01 * x ** 2 + 0.1 * x\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1)\n",
    "y = function_1(x)\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return np.sum(x ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2\n",
    "\n",
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp2(x1):\n",
    "    return 3.0 ** 2 + x1 * x1\n",
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num = 100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x = init_x, lr = 0.1, step_num = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x = init_x, lr = 10, step_num = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x = init_x, lr = 1e-10, step_num = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functions import softmax, cross_entropy_error\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3)\n",
    "        \n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.61386309 1.00849975 0.72168031]\n",
      " [1.00892773 1.39487228 1.1352884 ]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.27635281, 1.8604849 , 1.45476775])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.205063876181728"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0,0,1])\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15042283,  0.26977338, -0.42019621],\n",
       "       [ 0.22563424,  0.40466007, -0.63029431]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functions import *\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        self.params = {}\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params[\"b1\"] = np.zeros(hidden_size)\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params[\"b2\"] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params[\"W1\"], self.params[\"W2\"]\n",
    "        b1, b2 = self.params[\"b1\"], self.params[\"b2\"]\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(s.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W : self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads[\"W1\"] = numerical_gradient(loss_W, self.params[\"W1\"])\n",
    "        grads[\"b1\"] = numerical_gradient(loss_W, self.params[\"b1\"])\n",
    "        grads[\"W2\"] = numerical_gradient(loss_W, self.params[\"W2\"])\n",
    "        grads[\"b2\"] = numerical_gradient(loss_W, self.params[\"b2\"])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\n",
    "net.params[\"W1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params[\"b1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params[\"W2\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.params[\"b2\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09756306, 0.10031272, 0.10060266, 0.10135859, 0.10212724,\n",
       "        0.09467993, 0.09929564, 0.10862662, 0.09711601, 0.09831756],\n",
       "       [0.09721038, 0.10020483, 0.10065244, 0.10180472, 0.10207986,\n",
       "        0.09535591, 0.09908263, 0.10857265, 0.09701099, 0.09802559],\n",
       "       [0.09743988, 0.09984469, 0.10075639, 0.1016127 , 0.10200045,\n",
       "        0.0950496 , 0.09891259, 0.10919225, 0.09713068, 0.09806078],\n",
       "       [0.0967377 , 0.10033565, 0.10040582, 0.10155689, 0.10235177,\n",
       "        0.09502801, 0.09935367, 0.10886901, 0.09709517, 0.09826632],\n",
       "       [0.0973941 , 0.09973954, 0.10062548, 0.10141941, 0.10207128,\n",
       "        0.09534529, 0.09941559, 0.10881483, 0.09694188, 0.09823259],\n",
       "       [0.09717085, 0.10005645, 0.10065445, 0.10155808, 0.1021083 ,\n",
       "        0.09506918, 0.09920629, 0.10917507, 0.09701652, 0.0979848 ],\n",
       "       [0.09714027, 0.10008482, 0.10072372, 0.10151185, 0.10225317,\n",
       "        0.09534213, 0.09917682, 0.10877587, 0.09704246, 0.0979489 ],\n",
       "       [0.09735696, 0.09991901, 0.10057397, 0.10151558, 0.10229837,\n",
       "        0.09501329, 0.09952838, 0.10862346, 0.09701111, 0.09815987],\n",
       "       [0.09767442, 0.10010741, 0.10044465, 0.10172437, 0.10215824,\n",
       "        0.09498014, 0.09922465, 0.10855378, 0.09694291, 0.09818944],\n",
       "       [0.09724607, 0.10022085, 0.10016195, 0.10184413, 0.102034  ,\n",
       "        0.09498234, 0.09913319, 0.10889216, 0.09727416, 0.09821116],\n",
       "       [0.09700556, 0.09998876, 0.10088106, 0.1019133 , 0.10193192,\n",
       "        0.09512147, 0.09917176, 0.10891186, 0.09682525, 0.09824906],\n",
       "       [0.09711652, 0.10018141, 0.10091892, 0.10163177, 0.10182629,\n",
       "        0.0952798 , 0.09926387, 0.10881159, 0.09705318, 0.09791665],\n",
       "       [0.09732564, 0.10028395, 0.10064652, 0.10195888, 0.10182927,\n",
       "        0.09481054, 0.09915601, 0.1089364 , 0.09701834, 0.09803444],\n",
       "       [0.09709855, 0.09994817, 0.10070464, 0.10161994, 0.10258695,\n",
       "        0.09497911, 0.09947323, 0.10854142, 0.09690154, 0.09814645],\n",
       "       [0.09752392, 0.10017685, 0.10070738, 0.1015342 , 0.10177408,\n",
       "        0.0955122 , 0.09907815, 0.1086429 , 0.09706664, 0.09798368],\n",
       "       [0.09770573, 0.1002243 , 0.1004509 , 0.10153212, 0.1024272 ,\n",
       "        0.0949225 , 0.09908959, 0.10886039, 0.09697333, 0.09781394],\n",
       "       [0.09751642, 0.09977453, 0.10066164, 0.10143039, 0.10215425,\n",
       "        0.09542068, 0.09900263, 0.10879758, 0.09753103, 0.09771087],\n",
       "       [0.0972272 , 0.09998718, 0.10070384, 0.10184283, 0.10261551,\n",
       "        0.09506351, 0.09905139, 0.10896674, 0.09685012, 0.09769167],\n",
       "       [0.0973371 , 0.10025939, 0.10019214, 0.1017345 , 0.10216445,\n",
       "        0.09523587, 0.09930033, 0.10901378, 0.09676461, 0.09799784],\n",
       "       [0.09714238, 0.10004695, 0.10084063, 0.1015334 , 0.10239972,\n",
       "        0.09494537, 0.09901528, 0.10877664, 0.09722669, 0.09807294],\n",
       "       [0.09724774, 0.0999185 , 0.10073458, 0.10156602, 0.10240104,\n",
       "        0.09489286, 0.09929048, 0.10890495, 0.09699985, 0.09804397],\n",
       "       [0.09751292, 0.10010648, 0.1002922 , 0.10190894, 0.10221029,\n",
       "        0.09533261, 0.09901494, 0.10841654, 0.09728326, 0.09792181],\n",
       "       [0.09722325, 0.1001946 , 0.10061737, 0.10167567, 0.1023424 ,\n",
       "        0.09486957, 0.0991637 , 0.10897444, 0.09690376, 0.09803525],\n",
       "       [0.0976166 , 0.10035841, 0.10024161, 0.10169851, 0.10197515,\n",
       "        0.09505246, 0.09916052, 0.10849304, 0.09705519, 0.09834851],\n",
       "       [0.09752608, 0.09963741, 0.10089299, 0.10168155, 0.10238555,\n",
       "        0.0950786 , 0.09928705, 0.1085408 , 0.09699169, 0.09797826],\n",
       "       [0.09776446, 0.10011768, 0.10040235, 0.10171374, 0.10183327,\n",
       "        0.09489104, 0.09925715, 0.10854348, 0.09726726, 0.09820958],\n",
       "       [0.09713476, 0.09983448, 0.10062101, 0.10156018, 0.10222168,\n",
       "        0.09545763, 0.09946805, 0.1084542 , 0.09717338, 0.09807462],\n",
       "       [0.09732795, 0.10005859, 0.10042389, 0.10151461, 0.10246268,\n",
       "        0.09537763, 0.09936898, 0.10841291, 0.09682971, 0.09822305],\n",
       "       [0.09737787, 0.09975464, 0.10111507, 0.10123125, 0.10241413,\n",
       "        0.09513073, 0.09947647, 0.10872897, 0.09686632, 0.09790455],\n",
       "       [0.09721826, 0.10016611, 0.10073203, 0.1019267 , 0.10211298,\n",
       "        0.0951033 , 0.09905049, 0.10854256, 0.09714543, 0.09800214],\n",
       "       [0.09761014, 0.10014779, 0.10038122, 0.10159498, 0.10210972,\n",
       "        0.09489774, 0.09921236, 0.10893402, 0.09708697, 0.09802507],\n",
       "       [0.09739797, 0.09980386, 0.10068961, 0.10163528, 0.1019888 ,\n",
       "        0.09501991, 0.09933049, 0.10917364, 0.096683  , 0.09827743],\n",
       "       [0.09759948, 0.10046967, 0.10083367, 0.10180154, 0.10207977,\n",
       "        0.09476043, 0.09924083, 0.10821862, 0.09708489, 0.0979111 ],\n",
       "       [0.09747864, 0.09995197, 0.10073282, 0.10181449, 0.10199853,\n",
       "        0.09508703, 0.09876971, 0.10878267, 0.09723187, 0.09815227],\n",
       "       [0.09737878, 0.10004084, 0.10086996, 0.10181978, 0.10207975,\n",
       "        0.09517584, 0.09891508, 0.10866425, 0.09697432, 0.0980814 ],\n",
       "       [0.09713835, 0.10010883, 0.10042237, 0.10193904, 0.10192849,\n",
       "        0.09528155, 0.09880402, 0.10898559, 0.09717398, 0.09821778],\n",
       "       [0.09736463, 0.10002412, 0.10065766, 0.10157345, 0.10209103,\n",
       "        0.09515983, 0.09940058, 0.10894077, 0.09685001, 0.09793793],\n",
       "       [0.09737673, 0.10013879, 0.10039057, 0.10179662, 0.10240782,\n",
       "        0.09510882, 0.09934593, 0.10845754, 0.0968972 , 0.09807997],\n",
       "       [0.0975037 , 0.10001526, 0.10083501, 0.10179797, 0.10193279,\n",
       "        0.09507177, 0.09900787, 0.10889705, 0.09703546, 0.09790311],\n",
       "       [0.09717913, 0.09992024, 0.10082473, 0.10168584, 0.10217549,\n",
       "        0.09524971, 0.09931668, 0.10884671, 0.09673187, 0.09806959],\n",
       "       [0.09747719, 0.1002297 , 0.1008349 , 0.10174296, 0.10178187,\n",
       "        0.09506427, 0.09922191, 0.10865072, 0.09711026, 0.09788621],\n",
       "       [0.09748842, 0.1000728 , 0.10038794, 0.1014909 , 0.10219065,\n",
       "        0.0952026 , 0.0995405 , 0.10859827, 0.09687291, 0.09815501],\n",
       "       [0.09719855, 0.09980076, 0.10066997, 0.10165546, 0.10218642,\n",
       "        0.09521769, 0.09944321, 0.10873376, 0.0969117 , 0.09818248],\n",
       "       [0.09749369, 0.09991143, 0.10060706, 0.10184865, 0.1019813 ,\n",
       "        0.09522924, 0.09931419, 0.10853021, 0.09681556, 0.09826866],\n",
       "       [0.09707567, 0.10035922, 0.10078183, 0.10150504, 0.10218363,\n",
       "        0.0948914 , 0.09958602, 0.10869914, 0.09664587, 0.09827218],\n",
       "       [0.09747871, 0.10012407, 0.10065386, 0.10159223, 0.10223418,\n",
       "        0.09501368, 0.09951694, 0.10856765, 0.09674313, 0.09807556],\n",
       "       [0.09690156, 0.10043578, 0.1007447 , 0.10145254, 0.10231811,\n",
       "        0.0950626 , 0.09901106, 0.10901679, 0.09698937, 0.0980675 ],\n",
       "       [0.09733906, 0.10018117, 0.1004013 , 0.10173976, 0.10227568,\n",
       "        0.09495619, 0.09916965, 0.10884232, 0.09721577, 0.09787909],\n",
       "       [0.09695292, 0.10007547, 0.10070975, 0.10163318, 0.1021838 ,\n",
       "        0.09549931, 0.09912895, 0.10856732, 0.09707066, 0.09817864],\n",
       "       [0.09752725, 0.09967774, 0.10036288, 0.101603  , 0.10248137,\n",
       "        0.09526168, 0.0995432 , 0.10855842, 0.09693319, 0.09805127],\n",
       "       [0.09728639, 0.0999289 , 0.10079943, 0.10184978, 0.10212461,\n",
       "        0.09497086, 0.09921352, 0.10864041, 0.09721352, 0.09797258],\n",
       "       [0.09754309, 0.10041155, 0.10054512, 0.10167838, 0.10217202,\n",
       "        0.09501426, 0.09909628, 0.10866357, 0.09697819, 0.09789755],\n",
       "       [0.09765363, 0.09996018, 0.10070676, 0.10175182, 0.10192162,\n",
       "        0.09522767, 0.09880451, 0.10871793, 0.09729667, 0.09795921],\n",
       "       [0.09717851, 0.10031875, 0.10055626, 0.10167704, 0.10228997,\n",
       "        0.0949466 , 0.09884389, 0.10888627, 0.09702757, 0.09827515],\n",
       "       [0.09717368, 0.10033074, 0.10054184, 0.10179185, 0.10211189,\n",
       "        0.09487201, 0.09915229, 0.10877766, 0.09696939, 0.09827864],\n",
       "       [0.09719546, 0.09987177, 0.10016776, 0.10198728, 0.10223288,\n",
       "        0.09540195, 0.0993402 , 0.10885967, 0.09689726, 0.09804577],\n",
       "       [0.09729119, 0.10016777, 0.10079552, 0.10184554, 0.10212422,\n",
       "        0.09500221, 0.09923738, 0.1083697 , 0.09687577, 0.09829069],\n",
       "       [0.0975201 , 0.10014155, 0.10081069, 0.10130537, 0.10205646,\n",
       "        0.09487097, 0.09955656, 0.10869632, 0.09668871, 0.09835327],\n",
       "       [0.09728937, 0.10015834, 0.10074678, 0.10185578, 0.10180777,\n",
       "        0.09511757, 0.0992078 , 0.1087312 , 0.09675105, 0.09833435],\n",
       "       [0.09755382, 0.09992644, 0.10070161, 0.10148094, 0.1021105 ,\n",
       "        0.09529973, 0.09908553, 0.10881651, 0.09698897, 0.09803597],\n",
       "       [0.0973083 , 0.10031082, 0.10043647, 0.10144909, 0.10210067,\n",
       "        0.09505961, 0.09938585, 0.10866054, 0.09711025, 0.09817839],\n",
       "       [0.09748793, 0.10021963, 0.10040571, 0.10150663, 0.10215655,\n",
       "        0.09527582, 0.09914245, 0.10865517, 0.09717049, 0.09797962],\n",
       "       [0.09748492, 0.10007544, 0.10070243, 0.10175958, 0.10205741,\n",
       "        0.09498436, 0.09921358, 0.10870267, 0.09683794, 0.09818167],\n",
       "       [0.0974547 , 0.09967449, 0.10006481, 0.10175978, 0.10251495,\n",
       "        0.09515051, 0.0994637 , 0.10863918, 0.09732963, 0.09794824],\n",
       "       [0.0970017 , 0.09999009, 0.10085213, 0.10189452, 0.10242206,\n",
       "        0.09525322, 0.09893752, 0.10863185, 0.09699606, 0.09802084],\n",
       "       [0.09735831, 0.10013207, 0.10065001, 0.10154275, 0.10240687,\n",
       "        0.09513338, 0.09915061, 0.10851606, 0.09701083, 0.09809912],\n",
       "       [0.0975341 , 0.09994262, 0.10046776, 0.10187382, 0.10202587,\n",
       "        0.09479716, 0.09920271, 0.10856107, 0.09742508, 0.09816981],\n",
       "       [0.09752308, 0.09969907, 0.10047437, 0.10177582, 0.10191025,\n",
       "        0.0952833 , 0.09922896, 0.10892144, 0.09690864, 0.09827508],\n",
       "       [0.09700351, 0.09999657, 0.1002927 , 0.10164042, 0.10226973,\n",
       "        0.09522418, 0.09937168, 0.10910146, 0.09692729, 0.09817245],\n",
       "       [0.09730381, 0.10040154, 0.10066033, 0.10178651, 0.10218211,\n",
       "        0.09477775, 0.09920179, 0.10847314, 0.09715819, 0.09805483],\n",
       "       [0.09765919, 0.10024152, 0.10031696, 0.10144204, 0.102337  ,\n",
       "        0.09491892, 0.0991805 , 0.10867316, 0.09722638, 0.09800432],\n",
       "       [0.09748318, 0.10034976, 0.10083714, 0.1015948 , 0.10191785,\n",
       "        0.09510494, 0.09903989, 0.10852561, 0.09710822, 0.09803861],\n",
       "       [0.09738099, 0.10027964, 0.10084094, 0.10225654, 0.10216001,\n",
       "        0.09474185, 0.09917626, 0.10842603, 0.09689518, 0.09784255],\n",
       "       [0.09710905, 0.10031343, 0.10090106, 0.10167217, 0.10191988,\n",
       "        0.09522393, 0.09931177, 0.10870247, 0.09683125, 0.09801501],\n",
       "       [0.09752337, 0.10009717, 0.10086747, 0.10146102, 0.10213273,\n",
       "        0.09504315, 0.09903281, 0.10878845, 0.09697404, 0.09807978],\n",
       "       [0.09762188, 0.10036813, 0.09997817, 0.10174776, 0.10228596,\n",
       "        0.09485969, 0.09927378, 0.10896943, 0.09700573, 0.09788947],\n",
       "       [0.09712578, 0.09978409, 0.10058035, 0.10160228, 0.10240825,\n",
       "        0.09515716, 0.09955116, 0.1085153 , 0.09720406, 0.09807158],\n",
       "       [0.09705392, 0.09978799, 0.10029632, 0.10228202, 0.1023889 ,\n",
       "        0.09544801, 0.09912316, 0.1086059 , 0.09700859, 0.09800517],\n",
       "       [0.09741038, 0.09999141, 0.10046636, 0.10150929, 0.10246998,\n",
       "        0.09522716, 0.09931057, 0.10845443, 0.09713238, 0.09802804],\n",
       "       [0.09765013, 0.10014769, 0.10046465, 0.10160705, 0.10200139,\n",
       "        0.09534599, 0.09879584, 0.10844133, 0.0973215 , 0.09822444],\n",
       "       [0.09756918, 0.09986909, 0.10052604, 0.10136693, 0.1021729 ,\n",
       "        0.0953192 , 0.09925735, 0.10901154, 0.0969331 , 0.09797468],\n",
       "       [0.09746497, 0.10032979, 0.10055705, 0.10197389, 0.10167759,\n",
       "        0.09521445, 0.0990621 , 0.10840065, 0.09718046, 0.09813904],\n",
       "       [0.09729642, 0.10028333, 0.10043833, 0.1015511 , 0.10254014,\n",
       "        0.09504937, 0.09918636, 0.10844123, 0.09692793, 0.09828579],\n",
       "       [0.09744691, 0.10039915, 0.1005286 , 0.10179748, 0.10187426,\n",
       "        0.09497323, 0.09913366, 0.10881012, 0.09708019, 0.0979564 ],\n",
       "       [0.09731922, 0.10007583, 0.10072612, 0.10173866, 0.10196986,\n",
       "        0.09502974, 0.09939304, 0.10877508, 0.09709332, 0.09787914],\n",
       "       [0.09760975, 0.09989291, 0.10041522, 0.10156268, 0.10200567,\n",
       "        0.09518591, 0.09938423, 0.10899573, 0.09691597, 0.09803192],\n",
       "       [0.09742262, 0.1001131 , 0.10054502, 0.10158072, 0.10198598,\n",
       "        0.09513808, 0.09927868, 0.10897215, 0.09700817, 0.09795548],\n",
       "       [0.09727112, 0.09954187, 0.10058936, 0.10184113, 0.10247979,\n",
       "        0.09496822, 0.09899157, 0.10902093, 0.09719139, 0.09810461],\n",
       "       [0.09751796, 0.09986872, 0.10050017, 0.10176632, 0.10212828,\n",
       "        0.09530631, 0.099461  , 0.1086472 , 0.09675736, 0.09804668],\n",
       "       [0.09725533, 0.10006564, 0.10059434, 0.1017692 , 0.10217866,\n",
       "        0.09516437, 0.0990793 , 0.10869899, 0.09705976, 0.09813441],\n",
       "       [0.09718504, 0.10034913, 0.10052498, 0.10170969, 0.10229274,\n",
       "        0.09487159, 0.0994637 , 0.10842392, 0.09710951, 0.09806972],\n",
       "       [0.09740156, 0.10028137, 0.1008661 , 0.10138704, 0.10236122,\n",
       "        0.0947641 , 0.09934199, 0.10846383, 0.09690723, 0.09822556],\n",
       "       [0.09711635, 0.10006285, 0.10103073, 0.10166484, 0.10245296,\n",
       "        0.09496834, 0.09906768, 0.10900629, 0.0965043 , 0.09812565],\n",
       "       [0.09752779, 0.09978553, 0.10045954, 0.10164687, 0.10209094,\n",
       "        0.09536595, 0.09927429, 0.10878499, 0.09712297, 0.09794113],\n",
       "       [0.09717098, 0.09979245, 0.10053625, 0.10167546, 0.10230122,\n",
       "        0.09517299, 0.09927515, 0.10905767, 0.09707789, 0.09793995],\n",
       "       [0.09715825, 0.10001814, 0.10057086, 0.10166953, 0.10254868,\n",
       "        0.09505234, 0.09932213, 0.10862327, 0.09683271, 0.0982041 ],\n",
       "       [0.09733921, 0.10023656, 0.10077045, 0.10141087, 0.10213542,\n",
       "        0.09505483, 0.09925051, 0.1085177 , 0.09706724, 0.09821721],\n",
       "       [0.09743576, 0.09996106, 0.1005254 , 0.10189482, 0.10236085,\n",
       "        0.0949034 , 0.09915723, 0.10864276, 0.09718688, 0.09793182],\n",
       "       [0.09735171, 0.10023296, 0.10070493, 0.10149157, 0.10252833,\n",
       "        0.09502362, 0.09928501, 0.10850166, 0.09708784, 0.09779237],\n",
       "       [0.09721271, 0.09985254, 0.10034144, 0.10160183, 0.10268668,\n",
       "        0.09485622, 0.09944495, 0.10886011, 0.0970448 , 0.09809872]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "y = net.predict(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "t = np.random.rand(100, 10)\n",
    "\n",
    "grads = net.numerical_gradient(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[\"W1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[\"b1\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[\"W2\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[\"b2\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize = True, one_hot_label = True)\n",
    "\n",
    "iters_num = 1\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    for key in (\"W1\", \"b1\", \"W2\", \"b2\"):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlplotlib.plplot as plt\n",
    "%matplotlib inline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
